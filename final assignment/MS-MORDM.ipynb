{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashok\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import (unicode_literals, print_function, absolute_import,\n",
    "                        division)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from ema_workbench import (Model, MultiprocessingEvaluator,\n",
    "                           ScalarOutcome, IntegerParameter, optimize, Scenario)\n",
    "from ema_workbench.em_framework.optimization import EpsilonProgress, HyperVolume\n",
    "from ema_workbench.util import ema_logging\n",
    "from ema_workbench import (Model, RealParameter, IntegerParameter, CategoricalParameter, ScalarOutcome, Constant)\n",
    "\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ema_workbench.em_framework.samplers import sample_levers, sample_uncertainties\n",
    "from ema_workbench.em_framework.evaluators import LHS, SOBOL, MORRIS, SequentialEvaluator\n",
    "\n",
    "#from __future__ import (unicode_literals, print_function, absolute_import,\n",
    "            #            division)\n",
    "\n",
    "\n",
    "from ema_workbench import (Model, MultiprocessingEvaluator,\n",
    "                           ScalarOutcome, IntegerParameter, optimize, Scenario)\n",
    "from ema_workbench.em_framework.optimization import EpsilonProgress, HyperVolume\n",
    "from ema_workbench.util import ema_logging\n",
    "\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discount rate 0\n",
      "discount rate 1\n",
      "discount rate 2\n",
      "A.0_ID flood wave shape\n",
      "A.1_Bmax\n",
      "A.1_pfail\n",
      "A.1_Brate\n",
      "A.2_Bmax\n",
      "A.2_pfail\n",
      "A.2_Brate\n",
      "A.3_Bmax\n",
      "A.3_pfail\n",
      "A.3_Brate\n",
      "A.4_Bmax\n",
      "A.4_pfail\n",
      "A.4_Brate\n",
      "A.5_Bmax\n",
      "A.5_pfail\n",
      "A.5_Brate\n",
      "A1.Bmax\n",
      "A1.Brate\n",
      "A1.pfail\n",
      "A2.Bmax\n",
      "A2.Brate\n",
      "A2.pfail\n",
      "A3.Bmax\n",
      "A3.Brate\n",
      "A3.pfail\n",
      "A4.Bmax\n",
      "A4.Brate\n",
      "A4.pfail\n",
      "A5.Bmax\n",
      "A5.Brate\n",
      "A5.pfail\n",
      "discount rate \n"
     ]
    }
   ],
   "source": [
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "dike_model = get_model_for_problem_formulation(1)\n",
    "\n",
    "reference_values = {'Bmax': 175, 'Brate': 1.5, 'pfail': 0.5,\n",
    "                    'discount rate': 3.5,\n",
    "                    'ID flood wave shape': 4}\n",
    "scen1 = {}\n",
    "\n",
    "\n",
    "    \n",
    "dike_model[0].uncertainties = [RealParameter('A.0_ID flood wave shape', 0, 133),\n",
    "                RealParameter('A1.Bmax', 30,350),\n",
    "                CategoricalParameter('A1.Brate',(0.9,1.5,1000)),\n",
    "                RealParameter('A1.pfail',0,1),\n",
    "                RealParameter('A2.Bmax', 30,350),\n",
    "                CategoricalParameter('A2.Brate',(0.9,1.5,1000)),\n",
    "                RealParameter('A2.pfail',0,1),\n",
    "                RealParameter('A3.Bmax', 30,350),\n",
    "                CategoricalParameter('A3.Brate',(0.9,1.5,1000)),\n",
    "                RealParameter('A3.pfail',0,1),\n",
    "                RealParameter('A4.Bmax', 30,350),\n",
    "                CategoricalParameter('A4.Brate',(0.9,1.5,1000)),\n",
    "                RealParameter('A4.pfail',0,1),\n",
    "                RealParameter('A5.Bmax', 30,350),\n",
    "                CategoricalParameter('A5.Brate',(0.9,1.5,1000)),\n",
    "                RealParameter('A5.pfail',0,1),\n",
    "                CategoricalParameter('discount rate ',(1.5,2.5,3.5,4.5))]\n",
    "\n",
    "\n",
    "# set levers\n",
    "dike_model[0].levers = [IntegerParameter('A1.DikeIncrease',0,10),\n",
    "                    IntegerParameter('A2.DikeIncrease',0,10),\n",
    "                    IntegerParameter('A3.DikeIncrease',0,10),\n",
    "                    IntegerParameter('A4.DikeIncrease',0,10),\n",
    "                    IntegerParameter('A5.DikeIncrease',0,2), #capped this variable to test if the model uses these specified levers\n",
    "                    CategoricalParameter('1_RfR',(0,1)),\n",
    "                    CategoricalParameter('2_RfR',(0,1)),\n",
    "                    CategoricalParameter('3_RfR',(0,1)),\n",
    "                    CategoricalParameter('4_RfR',(0,1)),\n",
    "                    CategoricalParameter('0_RfR',(0,1)),\n",
    "                    IntegerParameter('EWS_DaysToThreat',0,4)]\n",
    "\n",
    "\n",
    "#specify outcomes\n",
    "# note how we need to explicitely indicate the direction\n",
    "dike_model[0].outcomes = [ScalarOutcome('Expected Annual Damage', kind=ScalarOutcome.MINIMIZE),\n",
    "                ScalarOutcome('Total Investment Costs', kind=ScalarOutcome.MINIMIZE),\n",
    "                ScalarOutcome('Expected Number of Deaths', kind=ScalarOutcome.MINIMIZE)]\n",
    "#for key in dike_model[0].uncertainties:\n",
    "    #print(key)\n",
    "   # name_split = key.name.split('_')\n",
    "   # print(key.name)\n",
    "    #if len(name_split) == 1:\n",
    "        #scen1.update({key.name: reference_values[key.name]})\n",
    "\n",
    "    #else:\n",
    "        #scen1.update({key.name: reference_values[name_split[1]]})\n",
    "\n",
    "#ref_scenario = Scenario('reference', **scen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_metrics = [EpsilonProgress()]\n",
    "\n",
    "epsilon = [100000,100000,0.001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First MORDM Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started\n",
      "[MainProcess/INFO] generation 0: 0/1000 nfe\n"
     ]
    }
   ],
   "source": [
    "#something is wrong here -stuck on 0/1000\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "with MultiprocessingEvaluator(dike_model[0]) as evaluator:\n",
    "    results1000, convergence1000 = evaluator.optimize(nfe=1000, epsilons=epsilon, convergence=convergence_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plotting epsilon progress and hypervolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-770cb332c46c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreference_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBrate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'A2' is not defined"
     ]
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence10000.nfe, convergence10000.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.plot(convergence10000.nfe, convergence10000.hypervolume)\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-evaluate candidate soutions under uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From assignment 8\n",
    "\n",
    "from ema_workbench import Policy\n",
    "\n",
    "#Create a collection of all the policies\n",
    "new_policies = []\n",
    "\n",
    "#Iterate over the new results table\n",
    "for i in range(len(new_results)):\n",
    "    new_policy = Policy(name=\"Policy \" + str(i), \n",
    "                        c1 = new_results.iloc[i,0] , \n",
    "                        c2 = new_results.iloc[i,1], \n",
    "                        r1 = new_results.iloc[i,2], \n",
    "                        r2 = new_results.iloc[i,3], \n",
    "                        w1 = new_results.iloc[i,4])\n",
    "    new_policies.append(new_policy)\n",
    "\n",
    "new_policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 1000 scenarios for each of the policy options. Depending on how many solutions are left after implementing the constraint, consider using multiprocessing or ipyparallel to speed up calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ema_workbench.em_framework.model.Model at 0x1bc6a427940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "with MultiprocessingEvaluator(dike_model[0]) as evaluator:\n",
    "    new_experiments, new_outcomes = evaluator.perform_experiments(scenarios=1000, policies=new_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness metric Singal to Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_to_noise_ratio(np_array):\n",
    "    return np_array.mean() / np_array.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Robustness Dataframe\n",
    "robustness_metric = pd.DataFrame(columns = [\"Policy\", \"Max_P STNR\", \"Utility STNR\", \"Intertia STNR\", \"Reliability STNR\"])\n",
    "\n",
    "#Iterate over all policies\n",
    "for policy in new_experiments[\"policy\"].unique():\n",
    "    \n",
    "    #Get indices from the experiments from a specific policy\n",
    "    indices = new_experiments[new_experiments[\"policy\"] == policy].index\n",
    "\n",
    "    STN_Ratio_Max_P = signal_to_noise_ratio(new_outcomes[\"max_P\"][indices])\n",
    "    STN_Ratio_Utility = signal_to_noise_ratio(new_outcomes[\"utility\"][indices])\n",
    "    STN_Ratio_Interia = signal_to_noise_ratio(new_outcomes[\"inertia\"][indices])\n",
    "    STN_Ratio_Reliability = signal_to_noise_ratio(new_outcomes[\"reliability\"][indices])\n",
    "\n",
    "    #Add to the Metrics Dataframe\n",
    "    robustness_metric = robustness_metric.append(pd.DataFrame(index=np.arange(1), data ={\"Policy\" : policy, \"Max_P STNR\": STN_Ratio_Max_P, \"Utility STNR\": STN_Ratio_Utility, \"Intertia STNR\": STN_Ratio_Interia, \"Reliability STNR\": STN_Ratio_Reliability}))\n",
    "\n",
    "robustness_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "pd.plotting.parallel_coordinates(robustness_metric, \"Policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Scenario Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs some work\n",
    "from ema_workbench.analysis import prim\n",
    "\n",
    "x = new_experiments\n",
    "y = new_outcomes['utility'] < 0.25 #I select only the results with utility lower than 0.25\n",
    "prim_alg = prim.Prim(x, y, threshold=0.8) # must change the 0.8\n",
    "box1 = prim_alg.find_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_experiments = new_experiments.iloc[box1.yi]\n",
    "selected_outcomes = {k:v[box1.yi] for k,v in new_outcomes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_experiments.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results((selected_experiments, selected_outcomes), './results/results_MORDM.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.util.utilities import load_results\n",
    "selected_results = load_results('./results/results_MORDM.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The experiments are in the first index of selected results, while the outcomes are in the second index\n",
    "\n",
    "selected_experiments = selected_results[0]\n",
    "selected_outcomes = selected_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These results have to be combined into one large dataframe, which can be used to determine the reference variables of the policies\n",
    "\n",
    "selected_outcomes_DF = pd.DataFrame(selected_outcomes)\n",
    "selected_experiments\n",
    "\n",
    "# Concatenate into one large dataframe\n",
    "\n",
    "selected_dataframe = pd.concat([selected_experiments, selected_outcomes_DF], axis=1)\n",
    "\n",
    "print(selected_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the reference Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
