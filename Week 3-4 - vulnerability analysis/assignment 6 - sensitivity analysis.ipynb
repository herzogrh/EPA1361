{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPA1361 - Model-Based Decision Making\n",
    "# Week 3 - Sensitivity analysis\n",
    "\n",
    "This exercise uses the same predator-prey model we used for the multi-model exercise, focusing on the Python version. As with the other exercise, define a model object for the function below, with the uncertainty ranges provided:\n",
    "\n",
    "|Parameter\t|Range or value\t        |\n",
    "|-----------|--------------:|\n",
    "|prey_birth_rate    \t|0.015 – 0.035\t|\n",
    "|predation_rate|0.0005 – 0.003 \t|\n",
    "|predator_efficiency     \t|0.001 – 0.004\t    |\n",
    "|predator_loss_rate\t    |0.04 – 0.08\t    |\n",
    "\n",
    "* Sensitivity analysis often focuses on the final values of an outcome at the end of the simulation. However, we can also look at metrics that give us additional information about the behavior of the model over time. Using [the statsmodel library](https://www.statsmodels.org/stable/index.html) and an appropriate sampling design, fit a linear regression model for each of the following indicators. What can we conclude about the behavior of the model, and about the importance of the different inputs?\n",
    "\n",
    "  * The final values of the _prey_ outcome\n",
    "  * The mean values of the _prey_ outcome over time, within each experiment\n",
    "  * The standard deviations of the _prey_ outcome over time, within each experiment\n",
    "  \n",
    "\n",
    "* Use the Sobol sampling functionality included in the Workbench to perform experiments with a sample size of N=50, then analyze the results with SALib for the same three indicators. This requires specifying the keyword argument `'uncertainty_sampling'` of perform_experiments. Note that when using Sobol sampling, the meaning of the keyword argument `scenarios` changes a bit. In order to properly estimate Sobol scores as well as interaction effects, you require N * (2D+2) scenarios, where D is the number of uncertain parameters, and N is the value for scenarios passed to `perform_experiments`. Repeat the analysis for larger sample sizes, with N=250 and N=1000. How can we interpret the first-order and total indices? Are these sample sizes sufficient for a stable estimation of the indices? You'll need to use the [get_SALib_problem](https://emaworkbench.readthedocs.io/en/latest/ema_documentation/em_framework/salib_samplers.html) function to convert your Workbench experiments to a problem definition that you can pass to the SALib analysis function. \n",
    "\n",
    "* *hint*: sobol is a deterministic sequence of quasi random numbers. Thus, you can run with N=1000 and simply slice for 1:50 and 1:250.\n",
    "\n",
    "* Use the [Extra-Trees analysis](https://emaworkbench.readthedocs.io/en/latest/ema_documentation/analysis/feature_scoring.html) included in the Workbench to approximate the Sobol total indices, with a suitable sampling design. As a starting point, use an ensemble of 100 trees and a max_features parameter of 0.6, and set the analysis to regression mode. Are the estimated importances stable relative to the sample size and the analysis parameters? How do the results compare to the Sobol indices? For more details on this analysis see [Jaxa-Rozen & Kwakkel (2018)](https://www.sciencedirect.com/science/article/pii/S1364815217311581)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ema_workbench import (Model, RealParameter, TimeSeriesOutcome, perform_experiments, ema_logging, MultiprocessingEvaluator)\n",
    "\n",
    "from ema_workbench.em_framework.evaluators import LHS, SOBOL, MORRIS\n",
    "\n",
    "from ema_workbench.analysis import feature_scoring\n",
    "from ema_workbench.analysis.scenario_discovery_util import RuleInductionType\n",
    "from ema_workbench.em_framework.salib_samplers import get_SALib_problem\n",
    "from SALib.analyze import sobol\n",
    "\n",
    "from predprey_function import pred_prey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[MainProcess/INFO] pool started\n[MainProcess/INFO] performing 100 scenarios * 1 policies * 1 model(s) = 100 experiments\n[MainProcess/INFO] 10 cases completed\n[MainProcess/INFO] 20 cases completed\n[MainProcess/INFO] 30 cases completed\n[MainProcess/INFO] 40 cases completed\n[MainProcess/INFO] 50 cases completed\n[MainProcess/INFO] 60 cases completed\n[MainProcess/INFO] 70 cases completed\n[MainProcess/INFO] 80 cases completed\n[MainProcess/INFO] 90 cases completed\n[MainProcess/INFO] 100 cases completed\n[MainProcess/INFO] experiments finished\n[MainProcess/INFO] terminating pool\n"
    }
   ],
   "source": [
    "#Initiate Model\n",
    "model = Model(name=\"PredPreyModel\", function = pred_prey)\n",
    "\n",
    "#Define Uncertainties\n",
    "model.uncertainties = [RealParameter('prey_birth_rate', 0.015 , 0.035),\n",
    "                       RealParameter('predation_rate', 0.0005 , 0.003),\n",
    "                       RealParameter('predator_efficiency', 0.001 , 0.004),\n",
    "                       RealParameter('predator_loss_rate', 0.04 , 0.08)]\n",
    "\n",
    "#Define Outcomes\n",
    "model.outcomes = [TimeSeriesOutcome('TIME'),\n",
    "                  TimeSeriesOutcome('predators'),\n",
    "                  TimeSeriesOutcome('prey')]\n",
    "\n",
    "#Turn on logging\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "n_scenarios = 100\n",
    "\n",
    "with MultiprocessingEvaluator(model, n_processes=7) as evaluator:\n",
    "    experiments, outcomes = evaluator.perform_experiments(n_scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    predation_rate  predator_efficiency  predator_loss_rate  prey_birth_rate  \\\n0         0.001049             0.003816            0.057704         0.026930   \n1         0.002473             0.003945            0.044271         0.031024   \n2         0.001632             0.001864            0.049443         0.016611   \n3         0.001018             0.002644            0.059443         0.028831   \n4         0.001060             0.002268            0.047080         0.020310   \n..             ...                  ...                 ...              ...   \n95        0.002190             0.002515            0.045623         0.022981   \n96        0.000632             0.001657            0.071510         0.016978   \n97        0.002977             0.002084            0.054307         0.030349   \n98        0.000918             0.002867            0.047493         0.015631   \n99        0.001949             0.002533            0.043990         0.019777   \n\n   scenario policy          model  \n0         0   None  PredPreyModel  \n1         1   None  PredPreyModel  \n2         2   None  PredPreyModel  \n3         3   None  PredPreyModel  \n4         4   None  PredPreyModel  \n..      ...    ...            ...  \n95       95   None  PredPreyModel  \n96       96   None  PredPreyModel  \n97       97   None  PredPreyModel  \n98       98   None  PredPreyModel  \n99       99   None  PredPreyModel  \n\n[100 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>predation_rate</th>\n      <th>predator_efficiency</th>\n      <th>predator_loss_rate</th>\n      <th>prey_birth_rate</th>\n      <th>scenario</th>\n      <th>policy</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.001049</td>\n      <td>0.003816</td>\n      <td>0.057704</td>\n      <td>0.026930</td>\n      <td>0</td>\n      <td>None</td>\n      <td>PredPreyModel</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.002473</td>\n      <td>0.003945</td>\n      <td>0.044271</td>\n      <td>0.031024</td>\n      <td>1</td>\n      <td>None</td>\n      <td>PredPreyModel</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.001632</td>\n      <td>0.001864</td>\n      <td>0.049443</td>\n      <td>0.016611</td>\n      <td>2</td>\n      <td>None</td>\n      <td>PredPreyModel</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.001018</td>\n      <td>0.002644</td>\n      <td>0.059443</td>\n      <td>0.028831</td>\n      <td>3</td>\n      <td>None</td>\n      <td>PredPreyModel</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.001060</td>\n      <td>0.002268</td>\n      <td>0.047080</td>\n      <td>0.020310</td>\n      <td>4</td>\n      <td>None</td>\n      <td>PredPreyModel</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.002190</td>\n      <td>0.002515</td>\n      <td>0.045623</td>\n      <td>0.022981</td>\n      <td>95</td>\n      <td>None</td>\n      <td>PredPreyModel</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.000632</td>\n      <td>0.001657</td>\n      <td>0.071510</td>\n      <td>0.016978</td>\n      <td>96</td>\n      <td>None</td>\n      <td>PredPreyModel</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.002977</td>\n      <td>0.002084</td>\n      <td>0.054307</td>\n      <td>0.030349</td>\n      <td>97</td>\n      <td>None</td>\n      <td>PredPreyModel</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.000918</td>\n      <td>0.002867</td>\n      <td>0.047493</td>\n      <td>0.015631</td>\n      <td>98</td>\n      <td>None</td>\n      <td>PredPreyModel</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.001949</td>\n      <td>0.002533</td>\n      <td>0.043990</td>\n      <td>0.019777</td>\n      <td>99</td>\n      <td>None</td>\n      <td>PredPreyModel</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dtype('float64')"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "experiments_clean = experiments.drop(columns=['scenario','policy','model'])\n",
    "\n",
    "experiments_clean.values.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[50.        , 50.07428126, 50.1399309 , ..., 11.96575785,\n         12.04422527, 12.1232136 ]],\n\n       [[50.        , 49.76951906, 49.516566  , ...,  1.14016846,\n          1.14801712,  1.15592977]],\n\n       [[50.        , 49.79973565, 49.59582862, ..., 13.7287894 ,\n         13.77069132, 13.81281151]],\n\n       ...,\n\n       [[50.        , 49.63514998, 49.26374515, ..., 17.18059322,\n         16.94894584, 16.72207333]],\n\n       [[50.        , 49.96599795, 49.92652582, ...,  3.48517615,\n          3.46281986,  3.4409421 ]],\n\n       [[50.        , 49.76003808, 49.51121139, ...,  2.97698936,\n          2.98466666,  2.99242809]]])"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#create the right dataframe\n",
    "prey = outcomes['prey']\n",
    "prey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                      y   R-squared (uncentered):                   0.960\nModel:                            OLS   Adj. R-squared (uncentered):              0.958\nMethod:                 Least Squares   F-statistic:                              572.2\nDate:                Thu, 14 May 2020   Prob (F-statistic):                    5.07e-66\nTime:                        17:30:42   Log-Likelihood:                         -321.00\nNo. Observations:                 100   AIC:                                      650.0\nDf Residuals:                      96   BIC:                                      660.4\nDf Model:                           4                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1          -151.0666    887.394     -0.170      0.865   -1912.530    1610.397\nx2         -1.135e+04    674.646    -16.824      0.000   -1.27e+04      -1e+04\nx3           746.1606     40.993     18.202      0.000     664.791     827.530\nx4           404.4500     96.329      4.199      0.000     213.239     595.661\n==============================================================================\nOmnibus:                        2.351   Durbin-Watson:                   1.965\nProb(Omnibus):                  0.309   Jarque-Bera (JB):                1.864\nSkew:                           0.323   Prob(JB):                        0.394\nKurtosis:                       3.172   Cond. No.                         98.4\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.960</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.958</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   572.2</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 14 May 2020</td> <th>  Prob (F-statistic):</th>          <td>5.07e-66</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>17:30:42</td>     <th>  Log-Likelihood:    </th>          <td> -321.00</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th>          <td>   650.0</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    96</td>      <th>  BIC:               </th>          <td>   660.4</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>              <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>x1</th> <td> -151.0666</td> <td>  887.394</td> <td>   -0.170</td> <td> 0.865</td> <td>-1912.530</td> <td> 1610.397</td>\n</tr>\n<tr>\n  <th>x2</th> <td>-1.135e+04</td> <td>  674.646</td> <td>  -16.824</td> <td> 0.000</td> <td>-1.27e+04</td> <td>   -1e+04</td>\n</tr>\n<tr>\n  <th>x3</th> <td>  746.1606</td> <td>   40.993</td> <td>   18.202</td> <td> 0.000</td> <td>  664.791</td> <td>  827.530</td>\n</tr>\n<tr>\n  <th>x4</th> <td>  404.4500</td> <td>   96.329</td> <td>    4.199</td> <td> 0.000</td> <td>  213.239</td> <td>  595.661</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 2.351</td> <th>  Durbin-Watson:     </th> <td>   1.965</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.309</td> <th>  Jarque-Bera (JB):  </th> <td>   1.864</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.323</td> <th>  Prob(JB):          </th> <td>   0.394</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 3.172</td> <th>  Cond. No.          </th> <td>    98.4</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "prey_mean = np.mean(outcomes['prey'][:,0,:], axis=1)\n",
    "prey_mean\n",
    "\n",
    "LR_mean = statsmodels.regression.linear_model.OLS(prey_mean, experiments_clean.values)\n",
    "\n",
    "fitted_mean = LR_mean.fit()\n",
    "\n",
    "fitted_mean.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                      y   R-squared (uncentered):                   0.795\nModel:                            OLS   Adj. R-squared (uncentered):              0.787\nMethod:                 Least Squares   F-statistic:                              93.32\nDate:                Thu, 14 May 2020   Prob (F-statistic):                    3.25e-32\nTime:                        17:30:45   Log-Likelihood:                         -417.84\nNo. Observations:                 100   AIC:                                      843.7\nDf Residuals:                      96   BIC:                                      854.1\nDf Model:                           4                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1          -468.9496   2337.243     -0.201      0.841   -5108.340    4170.441\nx2         -1.361e+04   1776.902     -7.660      0.000   -1.71e+04   -1.01e+04\nx3           890.3617    107.967      8.247      0.000     676.048    1104.675\nx4           340.8602    253.714      1.343      0.182    -162.757     844.478\n==============================================================================\nOmnibus:                        6.476   Durbin-Watson:                   2.109\nProb(Omnibus):                  0.039   Jarque-Bera (JB):                6.557\nSkew:                           0.592   Prob(JB):                       0.0377\nKurtosis:                       2.583   Cond. No.                         98.4\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.795</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.787</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   93.32</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 14 May 2020</td> <th>  Prob (F-statistic):</th>          <td>3.25e-32</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>17:30:45</td>     <th>  Log-Likelihood:    </th>          <td> -417.84</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th>          <td>   843.7</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    96</td>      <th>  BIC:               </th>          <td>   854.1</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>              <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>x1</th> <td> -468.9496</td> <td> 2337.243</td> <td>   -0.201</td> <td> 0.841</td> <td>-5108.340</td> <td> 4170.441</td>\n</tr>\n<tr>\n  <th>x2</th> <td>-1.361e+04</td> <td> 1776.902</td> <td>   -7.660</td> <td> 0.000</td> <td>-1.71e+04</td> <td>-1.01e+04</td>\n</tr>\n<tr>\n  <th>x3</th> <td>  890.3617</td> <td>  107.967</td> <td>    8.247</td> <td> 0.000</td> <td>  676.048</td> <td> 1104.675</td>\n</tr>\n<tr>\n  <th>x4</th> <td>  340.8602</td> <td>  253.714</td> <td>    1.343</td> <td> 0.182</td> <td> -162.757</td> <td>  844.478</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 6.476</td> <th>  Durbin-Watson:     </th> <td>   2.109</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.039</td> <th>  Jarque-Bera (JB):  </th> <td>   6.557</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.592</td> <th>  Prob(JB):          </th> <td>  0.0377</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.583</td> <th>  Cond. No.          </th> <td>    98.4</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "prey_final = outcomes['prey'][:,:,-1].flatten()\n",
    "prey_final\n",
    "\n",
    "LR_final = statsmodels.regression.linear_model.OLS(prey_final, experiments_clean.values)\n",
    "\n",
    "fitted_final = LR_final.fit()\n",
    "\n",
    "\n",
    "fitted_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                      y   R-squared (uncentered):                   0.957\nModel:                            OLS   Adj. R-squared (uncentered):              0.955\nMethod:                 Least Squares   F-statistic:                              535.1\nDate:                Thu, 14 May 2020   Prob (F-statistic):                    1.10e-64\nTime:                        17:30:47   Log-Likelihood:                         -253.25\nNo. Observations:                 100   AIC:                                      514.5\nDf Residuals:                      96   BIC:                                      524.9\nDf Model:                           4                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1           760.7902    450.711      1.688      0.095    -133.864    1655.444\nx2          2017.2761    342.655      5.887      0.000    1337.111    2697.442\nx3            27.8725     20.820      1.339      0.184     -13.455      69.200\nx4           244.2474     48.926      4.992      0.000     147.131     341.364\n==============================================================================\nOmnibus:                        8.012   Durbin-Watson:                   1.908\nProb(Omnibus):                  0.018   Jarque-Bera (JB):               10.885\nSkew:                           0.365   Prob(JB):                      0.00433\nKurtosis:                       4.442   Cond. No.                         98.4\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.957</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.955</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   535.1</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 14 May 2020</td> <th>  Prob (F-statistic):</th>          <td>1.10e-64</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>17:30:47</td>     <th>  Log-Likelihood:    </th>          <td> -253.25</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th>          <td>   514.5</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    96</td>      <th>  BIC:               </th>          <td>   524.9</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>              <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>x1</th> <td>  760.7902</td> <td>  450.711</td> <td>    1.688</td> <td> 0.095</td> <td> -133.864</td> <td> 1655.444</td>\n</tr>\n<tr>\n  <th>x2</th> <td> 2017.2761</td> <td>  342.655</td> <td>    5.887</td> <td> 0.000</td> <td> 1337.111</td> <td> 2697.442</td>\n</tr>\n<tr>\n  <th>x3</th> <td>   27.8725</td> <td>   20.820</td> <td>    1.339</td> <td> 0.184</td> <td>  -13.455</td> <td>   69.200</td>\n</tr>\n<tr>\n  <th>x4</th> <td>  244.2474</td> <td>   48.926</td> <td>    4.992</td> <td> 0.000</td> <td>  147.131</td> <td>  341.364</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 8.012</td> <th>  Durbin-Watson:     </th> <td>   1.908</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.018</td> <th>  Jarque-Bera (JB):  </th> <td>  10.885</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.365</td> <th>  Prob(JB):          </th> <td> 0.00433</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 4.442</td> <th>  Cond. No.          </th> <td>    98.4</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "prey_std = np.std(outcomes['prey'][:,0,:], axis=1)\n",
    "prey_std\n",
    "\n",
    "LR_std = statsmodels.regression.linear_model.OLS(prey_std, experiments_clean.values)\n",
    "\n",
    "fitted_std = LR_std.fit()\n",
    "\n",
    "\n",
    "fitted_std.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOBOL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started\n",
      "[MainProcess/INFO] performing 5000 scenarios * 1 policies * 1 model(s) = 5000 experiments\n",
      "[MainProcess/INFO] 500 cases completed\n",
      "[MainProcess/INFO] 1000 cases completed\n",
      "[MainProcess/INFO] 1500 cases completed\n",
      "[MainProcess/INFO] 2000 cases completed\n",
      "[MainProcess/INFO] 2500 cases completed\n",
      "[MainProcess/INFO] 3000 cases completed\n",
      "[MainProcess/INFO] 3500 cases completed\n",
      "[MainProcess/INFO] 4000 cases completed\n",
      "[MainProcess/INFO] 4500 cases completed\n",
      "[MainProcess/INFO] 5000 cases completed\n",
      "[MainProcess/INFO] experiments finished\n",
      "[MainProcess/INFO] terminating pool\n"
     ]
    }
   ],
   "source": [
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    sa_results = evaluator.perform_experiments(scenarios=500,\n",
    "                                               uncertainty_sampling='sobol')\n",
    "\n",
    "experiments, outcomes = sa_results\n",
    "\n",
    "problem = get_SALib_problem(model.uncertainties)\n",
    "\n",
    "\n",
    "#Si = sobol.analyze(problem, outcomes['predation_rate'],\n",
    "                  # calc_second_order=True, print_to_console=False)\n",
    "#Si = sobol.analyze(problem, outcomes['predator_efficiency'],\n",
    "                 #  calc_second_order=True, print_to_console=False)\n",
    "#Si = sobol.analyze(problem, outcomes['predator_loss_rate'],\n",
    "                 #  calc_second_order=True, print_to_console=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1, 1461)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes[\"prey\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prey_mean = np.mean(outcomes['prey'][:,0,:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Si = sobol.analyze(problem, prey_mean,\n",
    "                   calc_second_order=True, print_to_console=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_filtered = {k:Si[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "Si_df = pd.DataFrame(scores_filtered, index=problem['names'])\n",
    "\n",
    "sns.set_style('white')\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "indices = Si_df[['S1','ST']]\n",
    "err = Si_df[['S1_conf','ST_conf']]\n",
    "\n",
    "indices.plot.bar(yerr=err.values.T,ax=ax)\n",
    "fig.set_size_inches(8,6)\n",
    "fig.subplots_adjust(bottom=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predation_rate         0.005425\n",
       "predator_efficiency    0.828010\n",
       "predator_loss_rate     0.215999\n",
       "prey_birth_rate        0.010643\n",
       "Name: ST, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[\"ST\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-164ff2525f25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeature_scoring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_scores_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ST'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mprey_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_scoring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRuleInductionType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREGRESSION\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnr_trees\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ema_workbench\\analysis\\feature_scoring.py\u001b[0m in \u001b[0;36mget_feature_scores_all\u001b[1;34m(x, y, alg, mode, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     '''\n\u001b[0;32m    349\u001b[0m     \u001b[0mcomplete\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "feature_scoring.get_feature_scores_all(x=indices['ST'], y= prey_mean, mode=feature_scoring.RuleInductionType.REGRESSION, max_features=0.6, nr_trees=100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predation_rate</th>\n",
       "      <th>predator_efficiency</th>\n",
       "      <th>predator_loss_rate</th>\n",
       "      <th>prey_birth_rate</th>\n",
       "      <th>scenario</th>\n",
       "      <th>policy</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.060742</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>100</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.060742</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>101</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.060742</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>102</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.041836</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>103</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.060742</td>\n",
       "      <td>0.032988</td>\n",
       "      <td>104</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.041836</td>\n",
       "      <td>0.032988</td>\n",
       "      <td>105</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.041836</td>\n",
       "      <td>0.032988</td>\n",
       "      <td>106</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.060742</td>\n",
       "      <td>0.032988</td>\n",
       "      <td>107</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.041836</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>108</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.041836</td>\n",
       "      <td>0.032988</td>\n",
       "      <td>109</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.040742</td>\n",
       "      <td>0.018535</td>\n",
       "      <td>110</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.040742</td>\n",
       "      <td>0.018535</td>\n",
       "      <td>111</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.040742</td>\n",
       "      <td>0.018535</td>\n",
       "      <td>112</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.061836</td>\n",
       "      <td>0.018535</td>\n",
       "      <td>113</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.040742</td>\n",
       "      <td>0.022988</td>\n",
       "      <td>114</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.061836</td>\n",
       "      <td>0.022988</td>\n",
       "      <td>115</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.061836</td>\n",
       "      <td>0.022988</td>\n",
       "      <td>116</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.040742</td>\n",
       "      <td>0.022988</td>\n",
       "      <td>117</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.061836</td>\n",
       "      <td>0.018535</td>\n",
       "      <td>118</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.061836</td>\n",
       "      <td>0.022988</td>\n",
       "      <td>119</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.070742</td>\n",
       "      <td>0.033535</td>\n",
       "      <td>120</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.070742</td>\n",
       "      <td>0.033535</td>\n",
       "      <td>121</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.070742</td>\n",
       "      <td>0.033535</td>\n",
       "      <td>122</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.051836</td>\n",
       "      <td>0.033535</td>\n",
       "      <td>123</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.070742</td>\n",
       "      <td>0.017988</td>\n",
       "      <td>124</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.051836</td>\n",
       "      <td>0.017988</td>\n",
       "      <td>125</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.051836</td>\n",
       "      <td>0.017988</td>\n",
       "      <td>126</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.070742</td>\n",
       "      <td>0.017988</td>\n",
       "      <td>127</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.051836</td>\n",
       "      <td>0.033535</td>\n",
       "      <td>128</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.051836</td>\n",
       "      <td>0.017988</td>\n",
       "      <td>129</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.052207</td>\n",
       "      <td>0.025635</td>\n",
       "      <td>5070</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.052207</td>\n",
       "      <td>0.025635</td>\n",
       "      <td>5071</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.052207</td>\n",
       "      <td>0.025635</td>\n",
       "      <td>5072</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.057402</td>\n",
       "      <td>0.025635</td>\n",
       "      <td>5073</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.052207</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>5074</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.057402</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>5075</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.057402</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>5076</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.052207</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>5077</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4978</th>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.057402</td>\n",
       "      <td>0.025635</td>\n",
       "      <td>5078</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.057402</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>5079</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.062207</td>\n",
       "      <td>0.020635</td>\n",
       "      <td>5080</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.062207</td>\n",
       "      <td>0.020635</td>\n",
       "      <td>5081</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.062207</td>\n",
       "      <td>0.020635</td>\n",
       "      <td>5082</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.067402</td>\n",
       "      <td>0.020635</td>\n",
       "      <td>5083</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.062207</td>\n",
       "      <td>0.018506</td>\n",
       "      <td>5084</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.067402</td>\n",
       "      <td>0.018506</td>\n",
       "      <td>5085</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.067402</td>\n",
       "      <td>0.018506</td>\n",
       "      <td>5086</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.062207</td>\n",
       "      <td>0.018506</td>\n",
       "      <td>5087</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.067402</td>\n",
       "      <td>0.020635</td>\n",
       "      <td>5088</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.067402</td>\n",
       "      <td>0.018506</td>\n",
       "      <td>5089</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.030635</td>\n",
       "      <td>5090</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.030635</td>\n",
       "      <td>5091</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.030635</td>\n",
       "      <td>5092</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.047402</td>\n",
       "      <td>0.030635</td>\n",
       "      <td>5093</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.028506</td>\n",
       "      <td>5094</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.047402</td>\n",
       "      <td>0.028506</td>\n",
       "      <td>5095</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.047402</td>\n",
       "      <td>0.028506</td>\n",
       "      <td>5096</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.028506</td>\n",
       "      <td>5097</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.047402</td>\n",
       "      <td>0.030635</td>\n",
       "      <td>5098</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.047402</td>\n",
       "      <td>0.028506</td>\n",
       "      <td>5099</td>\n",
       "      <td>None</td>\n",
       "      <td>PredPreyModel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      predation_rate  predator_efficiency  predator_loss_rate  \\\n",
       "0           0.001049             0.001290            0.060742   \n",
       "1           0.001201             0.001290            0.060742   \n",
       "2           0.001049             0.003722            0.060742   \n",
       "3           0.001049             0.001290            0.041836   \n",
       "4           0.001049             0.001290            0.060742   \n",
       "5           0.001049             0.003722            0.041836   \n",
       "6           0.001201             0.001290            0.041836   \n",
       "7           0.001201             0.003722            0.060742   \n",
       "8           0.001201             0.003722            0.041836   \n",
       "9           0.001201             0.003722            0.041836   \n",
       "10          0.002299             0.002790            0.040742   \n",
       "11          0.002451             0.002790            0.040742   \n",
       "12          0.002299             0.002222            0.040742   \n",
       "13          0.002299             0.002790            0.061836   \n",
       "14          0.002299             0.002790            0.040742   \n",
       "15          0.002299             0.002222            0.061836   \n",
       "16          0.002451             0.002790            0.061836   \n",
       "17          0.002451             0.002222            0.040742   \n",
       "18          0.002451             0.002222            0.061836   \n",
       "19          0.002451             0.002222            0.061836   \n",
       "20          0.002924             0.002040            0.070742   \n",
       "21          0.001826             0.002040            0.070742   \n",
       "22          0.002924             0.001472            0.070742   \n",
       "23          0.002924             0.002040            0.051836   \n",
       "24          0.002924             0.002040            0.070742   \n",
       "25          0.002924             0.001472            0.051836   \n",
       "26          0.001826             0.002040            0.051836   \n",
       "27          0.001826             0.001472            0.070742   \n",
       "28          0.001826             0.001472            0.051836   \n",
       "29          0.001826             0.001472            0.051836   \n",
       "...              ...                  ...                 ...   \n",
       "4970        0.002188             0.002557            0.052207   \n",
       "4971        0.001282             0.002557            0.052207   \n",
       "4972        0.002188             0.001617            0.052207   \n",
       "4973        0.002188             0.002557            0.057402   \n",
       "4974        0.002188             0.002557            0.052207   \n",
       "4975        0.002188             0.001617            0.057402   \n",
       "4976        0.001282             0.002557            0.057402   \n",
       "4977        0.001282             0.001617            0.052207   \n",
       "4978        0.001282             0.001617            0.057402   \n",
       "4979        0.001282             0.001617            0.057402   \n",
       "4980        0.002813             0.001807            0.062207   \n",
       "4981        0.000657             0.001807            0.062207   \n",
       "4982        0.002813             0.002367            0.062207   \n",
       "4983        0.002813             0.001807            0.067402   \n",
       "4984        0.002813             0.001807            0.062207   \n",
       "4985        0.002813             0.002367            0.067402   \n",
       "4986        0.000657             0.001807            0.067402   \n",
       "4987        0.000657             0.002367            0.062207   \n",
       "4988        0.000657             0.002367            0.067402   \n",
       "4989        0.000657             0.002367            0.067402   \n",
       "4990        0.001563             0.003307            0.042207   \n",
       "4991        0.001907             0.003307            0.042207   \n",
       "4992        0.001563             0.003867            0.042207   \n",
       "4993        0.001563             0.003307            0.047402   \n",
       "4994        0.001563             0.003307            0.042207   \n",
       "4995        0.001563             0.003867            0.047402   \n",
       "4996        0.001907             0.003307            0.047402   \n",
       "4997        0.001907             0.003867            0.042207   \n",
       "4998        0.001907             0.003867            0.047402   \n",
       "4999        0.001907             0.003867            0.047402   \n",
       "\n",
       "      prey_birth_rate scenario policy          model  \n",
       "0            0.028535      100   None  PredPreyModel  \n",
       "1            0.028535      101   None  PredPreyModel  \n",
       "2            0.028535      102   None  PredPreyModel  \n",
       "3            0.028535      103   None  PredPreyModel  \n",
       "4            0.032988      104   None  PredPreyModel  \n",
       "5            0.032988      105   None  PredPreyModel  \n",
       "6            0.032988      106   None  PredPreyModel  \n",
       "7            0.032988      107   None  PredPreyModel  \n",
       "8            0.028535      108   None  PredPreyModel  \n",
       "9            0.032988      109   None  PredPreyModel  \n",
       "10           0.018535      110   None  PredPreyModel  \n",
       "11           0.018535      111   None  PredPreyModel  \n",
       "12           0.018535      112   None  PredPreyModel  \n",
       "13           0.018535      113   None  PredPreyModel  \n",
       "14           0.022988      114   None  PredPreyModel  \n",
       "15           0.022988      115   None  PredPreyModel  \n",
       "16           0.022988      116   None  PredPreyModel  \n",
       "17           0.022988      117   None  PredPreyModel  \n",
       "18           0.018535      118   None  PredPreyModel  \n",
       "19           0.022988      119   None  PredPreyModel  \n",
       "20           0.033535      120   None  PredPreyModel  \n",
       "21           0.033535      121   None  PredPreyModel  \n",
       "22           0.033535      122   None  PredPreyModel  \n",
       "23           0.033535      123   None  PredPreyModel  \n",
       "24           0.017988      124   None  PredPreyModel  \n",
       "25           0.017988      125   None  PredPreyModel  \n",
       "26           0.017988      126   None  PredPreyModel  \n",
       "27           0.017988      127   None  PredPreyModel  \n",
       "28           0.033535      128   None  PredPreyModel  \n",
       "29           0.017988      129   None  PredPreyModel  \n",
       "...               ...      ...    ...            ...  \n",
       "4970         0.025635     5070   None  PredPreyModel  \n",
       "4971         0.025635     5071   None  PredPreyModel  \n",
       "4972         0.025635     5072   None  PredPreyModel  \n",
       "4973         0.025635     5073   None  PredPreyModel  \n",
       "4974         0.023506     5074   None  PredPreyModel  \n",
       "4975         0.023506     5075   None  PredPreyModel  \n",
       "4976         0.023506     5076   None  PredPreyModel  \n",
       "4977         0.023506     5077   None  PredPreyModel  \n",
       "4978         0.025635     5078   None  PredPreyModel  \n",
       "4979         0.023506     5079   None  PredPreyModel  \n",
       "4980         0.020635     5080   None  PredPreyModel  \n",
       "4981         0.020635     5081   None  PredPreyModel  \n",
       "4982         0.020635     5082   None  PredPreyModel  \n",
       "4983         0.020635     5083   None  PredPreyModel  \n",
       "4984         0.018506     5084   None  PredPreyModel  \n",
       "4985         0.018506     5085   None  PredPreyModel  \n",
       "4986         0.018506     5086   None  PredPreyModel  \n",
       "4987         0.018506     5087   None  PredPreyModel  \n",
       "4988         0.020635     5088   None  PredPreyModel  \n",
       "4989         0.018506     5089   None  PredPreyModel  \n",
       "4990         0.030635     5090   None  PredPreyModel  \n",
       "4991         0.030635     5091   None  PredPreyModel  \n",
       "4992         0.030635     5092   None  PredPreyModel  \n",
       "4993         0.030635     5093   None  PredPreyModel  \n",
       "4994         0.028506     5094   None  PredPreyModel  \n",
       "4995         0.028506     5095   None  PredPreyModel  \n",
       "4996         0.028506     5096   None  PredPreyModel  \n",
       "4997         0.028506     5097   None  PredPreyModel  \n",
       "4998         0.030635     5098   None  PredPreyModel  \n",
       "4999         0.028506     5099   None  PredPreyModel  \n",
       "\n",
       "[5000 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}